= Workload Schedulability SLOs

include::partial$runbooks/contribution_note.adoc[]

[[canary]]
== Workload Canary

=== icon:glasses[] Overview

This SLO measures the percentage of pods timed out during a complete lifecycle, measured while waiting for a canary pods.
The error rate is a general indicator of cluster and workload health.

include::partial$runbooks/alert_types.adoc[]

=== icon:bug[] Steps for debugging

Unschedulable workloads often indicate resource exhaustion on a cluster but can have many root causes.

First check the `Events:` section of the `kubectl describe` output.

[source,bash]
----
kubectl describe pods -A -l "scheduler-canary-controller.appuio.io/instance"
----

The output should contain events why the pod isn't schedulable:

[source]
----
Events:
  Type     Reason            Age                From               Message
  ----     ------            ----               ----               -------
  Warning  FailedScheduling  46s (x1 over 56s)  default-scheduler  0/12 nodes are available: 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 3 node(s) had taint {storagenode: True}, that the pod didn't tolerate, 6 Insufficient cpu, 6 Insufficient memory.

Events:
  Type     Reason            Age                From               Message
  ----     ------            ----               ----               -------
  Warning  FailedScheduling  12s (x1 over 15s)  default-scheduler  0/12 nodes are available: 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 3 node(s) had taint {storagenode: True}, that the pod didn't tolerate, 6 Too many pods.
----

include::partial$runbooks/wip_note.adoc[]

==== Resource Exhaustion

Allocatable resources can be checked with the `kubectl describe nodes` output.

[source,bash]
----
kubectl --as=cluster-admin describe nodes -l node-role.kubernetes.io/app=
[...]
Allocatable:
  cpu:                3500m
  ephemeral-storage:  123201474766
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             15258372Ki
  pods:               110
[...]
Non-terminated Pods:                      (38 in total)
[...]
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests      Limits
  --------           --------      ------
  cpu                2644m (75%)   5420m (154%)
  memory             5914Mi (39%)  14076Mi (94%)
  ephemeral-storage  100Ki (0%)    0 (0%)
  hugepages-1Gi      0 (0%)        0 (0%)
  hugepages-2Mi      0 (0%)        0 (0%)
----

==== `ImagePullBackOff`

The canary pod uses an image from the OpenShift registry.
If the registry is unavailable, the pod will fail to start.

Check the `Status:` section of the `kubectl describe` output:

[source,bash]
----
kubectl -n appuio-openshift4-slos describe imagestream canary
----

Check if pods are crashing for the image registry:

[source,bash]
----
kubectl -n openshift-image-registry get pods
----

Check the logs for the operator and the registry:

[source,bash]
----
kubectl -n openshift-image-registry logs deployments/cluster-image-registry-operator

kubectl -n openshift-image-registry logs deployments/image-registry --all-containers
----

=== icon:wrench[] Tune

If this alert isn't actionable, noisy, or was raised too late you may want to tune the SLO.

You have the option tune the SLO through the component parameters.
You can modify the objective, disable the page or ticket alert, or completely disable the SLO.

The example below will set the SLO set the objective to 99.25% and disable the page alert.
This means this SLO won't alert on-call anymore.

[source,yaml]
----
slos:
  workload-schedulability:
    canary:
      objective: 99.25
      alerting:
        page_alert:
          enabled: false
----

include::partial$runbooks/objective_change_warning.adoc[]
