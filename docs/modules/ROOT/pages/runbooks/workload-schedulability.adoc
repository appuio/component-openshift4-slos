= Workload Schedulability SLOs

include::partial$runbooks/contribution_note.adoc[]

[[canary]]
== Workload Canary

=== icon:glasses[] Overview

This SLO measures the percentage of pods timed out or failed during a complete lifecycle, measured while waiting for a canary pods.
In the current implementation the image for the pod is pulled from the built-in OpenShift registry.

The error rate is a general indicator of cluster and workload health.

include::partial$runbooks/alert_types.adoc[]

=== icon:bug[] Steps for debugging

Unschedulable workloads often indicate resource exhaustion on a cluster but can have many root causes.
Since the canary pod uses an image from the built-in OpenShift registry, the alert can be triggered by a misbehaving image registry.

First check the `Events:` section of the `kubectl describe` output.

[source,bash]
----
kubectl describe pods -A -l "scheduler-canary-controller.appuio.io/instance"
----

The output should contain events why the pod isn't schedulable:

[source]
----
Events:
  Type     Reason            Age                From               Message
  ----     ------            ----               ----               -------
  Warning  FailedScheduling  46s (x1 over 56s)  default-scheduler  0/12 nodes are available: 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 3 node(s) had taint {storagenode: True}, that the pod didn't tolerate, 6 Insufficient cpu, 6 Insufficient memory.

Events:
  Type     Reason            Age                From               Message
  ----     ------            ----               ----               -------
  Warning  FailedScheduling  12s (x1 over 15s)  default-scheduler  0/12 nodes are available: 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 3 node(s) had taint {storagenode: True}, that the pod didn't tolerate, 6 Too many pods.

Events:
  Type     Reason          Age   From               Message
  ----     ------          ----  ----               -------
  Warning  Failed          13s   kubelet            Failed to pull image "invalid:image": rpc error: code = Unknown desc = reading manifest image in docker.io/library/invalid: errors:
denied: requested access to the resource is denied
unauthorized: authentication required
  Warning  Failed   13s               kubelet  Error: ErrImagePull
  Normal   BackOff  12s               kubelet  Back-off pulling image "invalid:image"
  Warning  Failed   12s               kubelet  Error: ImagePullBackOff
  Normal   Pulling  0s (x2 over 15s)  kubelet  Pulling image "invalid:image"
----

include::partial$runbooks/wip_note.adoc[]

==== Resource Exhaustion

Allocatable resources can be checked with the `kubectl describe nodes` output.

[source,bash]
----
kubectl --as=system:admin describe nodes -l node-role.kubernetes.io/app=
[...]
Allocatable:
  cpu:                3500m
  ephemeral-storage:  123201474766
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             15258372Ki
  pods:               110
[...]
Non-terminated Pods:                      (38 in total)
[...]
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests      Limits
  --------           --------      ------
  cpu                2644m (75%)   5420m (154%)
  memory             5914Mi (39%)  14076Mi (94%)
  ephemeral-storage  100Ki (0%)    0 (0%)
  hugepages-1Gi      0 (0%)        0 (0%)
  hugepages-2Mi      0 (0%)        0 (0%)
----

==== `ImagePullBackOff`

The canary pod uses an image from the OpenShift registry.
If the registry is unavailable, the pod will fail to start.

Check the `Status:` section of the `kubectl describe` output:

[source,bash]
----
kubectl -n appuio-openshift4-slos describe imagestream canary
----

Check if pods are crashing for the image registry:

[source,bash]
----
kubectl -n openshift-image-registry get pods
----

Check the logs for the operator and the registry:

[source,bash]
----
kubectl -n openshift-image-registry logs deployments/cluster-image-registry-operator

kubectl -n openshift-image-registry logs deployments/image-registry --all-containers
----

=== icon:wrench[] Tune

If this alert isn't actionable, noisy, or was raised too late you may want to tune the SLO.

You have the option tune the SLO through the component parameters.
You can modify the objective, disable the page or ticket alert, or completely disable the SLO.

The example below will set the SLO set the objective to 99.25%, adjust the time until a pod is seen as timed out, and disable the page alert.
This means this SLO won't alert on-call anymore.

[source,yaml]
----
slos:
  workload-schedulability:
    canary:
      objective: 99.25
      alerting:
        page_alert:
          enabled: false
      _sli:
        overallPodTimeout: 5m
----

include::partial$runbooks/objective_change_warning.adoc[]
